# GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation

<div align="center">

[![Paper](https://img.shields.io/badge/arXiv-2512.17495-b31b1b.svg)](https://arxiv.org/abs/2512.17495)
[![GitHub](https://img.shields.io/badge/GitHub-GroundingME-black?logo=github)](https://github.com/lirang04/GroundingME)
[![GroundingME](https://img.shields.io/badge/ğŸ¤—-GroundingME-yellow)](https://huggingface.co/datasets/lirang04/GroundingME)
[![GroundingME](https://img.shields.io/badge/ğŸ¤—-RefCOCOg_rej-yellow)](https://huggingface.co/datasets/lirang04/RefCOCOg_rej)
[![Project Page](https://img.shields.io/badge/ğŸŒ-Project%20Page-blue)](https://groundingme.github.io)

</div>

## ğŸ” Overview

Visual groundingâ€”localizing objects from natural language descriptionsâ€”represents a critical bridge between language and vision understanding. While multimodal large language models (MLLMs) achieve impressive scores on existing benchmarks, a fundamental question remains: **can MLLMs truly ground language in vision with human-like sophistication, or are they merely pattern-matching on simplified datasets?**

Current benchmarks fail to capture real-world complexity where humans effortlessly navigate ambiguous references and recognize when grounding is impossible. To rigorously assess MLLMs' true capabilities, we introduce **GroundingME**, a benchmark that systematically challenges models across four critical dimensions:

- ğŸ¯ **Discriminative** â€” Distinguishing highly similar objects
- ğŸ“ **Spatial** â€” Understanding complex relational descriptions  
- ğŸ”¬ **Limited** â€” Handling occlusions or tiny objects
- âŒ **Rejection** â€” Recognizing ungroundable queries

<p align="center">
  <img src="https://groundingme.github.io/images/examples.jpg" width="70%">
</p>


## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
cd groundingme

# Install dependencies
pip install datasets pillow tqdm openai
```

### Evaluate Your Model

The evaluation script supports any OpenAI-compatible API:

```bash
# Local vLLM server
python evaluate.py \
  --api-url http://localhost:8000/v1 \
  --api-key dummy \
  --model-name Qwen/Qwen3-VL-8B-Thinking \
  --workers 16 \
  --output results.json
```

### Load Dataset from HuggingFace

```python
from datasets import load_dataset

# Load the dataset
dataset = load_dataset("lirang04/GroundingME", split="test")

# Access a sample
sample = dataset[0]
image = sample["image"]
description = sample["description"]
bbox = sample["bbox"]  # Ground truth [x1, y1, x2, y2]
category = sample["subtask_l1"]  # Discriminative/Spatial/Limited/Rejection
```

## ğŸ“ˆ Benchmark Statistics

<p align="center">
  <img src="https://groundingme.github.io/images/category.jpg" width="60%">
</p>

## ğŸ“œ License

This benchmark follows the licensing terms of [SA-1B](https://ai.meta.com/datasets/segment-anything/) and [HR-Bench](https://huggingface.co/datasets/DreamMr/HR-Bench). **Research use only.**

## ğŸ“– Citation

If you find GroundingME useful for your research, please cite our paper:

```bibtex
@article{li2025groundingme,
  title={GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation},
  author={Li, Rang and Li, Lei and Ren, Shuhuai and Tian, Hao and Gu, Shuhao and Li, Shicheng and Yue, Zihao and Wang, Yudong and Ma, Wenhan and Yang, Zhe and others},
  journal={arXiv preprint arXiv:2512.17495},
  year={2025}
}
```

